<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p><br /></p>
<h2><span style="color:black;font-size:24pt;font-family:Songti SC"><b>Jiacheng Ye</b></span><br /></h2>
<table class="imgtable"><tr><td>
<img src="photos/bio.png" alt="alt text" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p><br />
Third-year Ph.D. student<br />
<a href="https://www.cs.hku.hk/">Department of Computer Science</a><br />
<a href="https://www.hku.hk/">The University of Hong Kong</a></p>
<p>[<a href="mailto:carsonye@connect.hku.hk">Email</a>]
[<a href="https://github.com/jiacheng-ye">Github</a>] [<a href="https://scholar.google.com/citations?user=gh0CyxgAAAAJ">Google Scholar</a>]</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a third-year Ph.D. student at the Department of Computer Science in the University of Hong Kong (HKU), supervised by <a href="https://ikekonglp.github.io/">Dr. Lingpeng Kong</a> and <a href="https://taoyds.github.io/">Dr. Tao Yu</a> in <a href="https://nlp.cs.hku.hk/">HKUNLP</a>. 
I received my Master degree in <a href="https://www.fudan.edu.cn/en/">Fudan University</a>, supervised by  <a href="http://www.qizhang.info/">Prof. Qi Zhang</a> in <a href="https://nlp.fudan.edu.cn/nlpen/main.htm">FudanNLP Group</a>,
and my Bachelor degree in <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>.</p>
<p>My recent research focuses on below topics:</p>
<ul>
<li><p>Text diffusion </p>
</li>
<li><p>Complex reasoning and planning</p>
</li>
<li><p>Interacting and understanding LLMs</p>
</li>
<li><p>Data synthesis principles</p>
</li>
</ul>
<h2>Publications </h2>
<p>(*: equal contribution)</p>
<p><b>Text Diffusion</b></p>
<ul>
<li><p><a href="https://openreview.net/pdf?id=A9y3LFX4ds">Implicit Search via Discrete Diffusion: A Study on Chess</a><br />
<b>Jiacheng Ye</b>, Zhenyu Wu, Jiahui Gao, Zhiyong Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong.<br />
ICLR 2025. [<a href="https://github.com/HKUNLP/DiffuSearch">code</a>] [<a href="https://lichess.org/diffusearchv0">chess-agent</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.14157">Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</a><br />
<b>Jiacheng Ye</b>, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong.<br />
ICLR 2025. [<a href="https://github.com/HKUNLP/diffusion-vs-ar">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.17891">Scaling Diffusion Language Models via Adaptation from Autoregressive Models</a><br />
Shansan Gong*, Shivam Agarwal*, Yizhe Zhang, <b>Jiacheng Ye</b>, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong.<br />
ICLR 2025. [<a href="https://github.com/HKUNLP/DiffuLLaMA">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.07754">Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a><br />
<b>Jiacheng Ye</b>*, Shansan Gong*, Liheng Chen*, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong.<br />
NeurIPS 2024. [<a href="https://github.com/HKUNLP/diffusion-of-thoughts">code</a>]</p>
</li>
</ul>
<p><b>In-context Learning</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.05698">Compositional Exemplars for In-context Learning</a><br />
<b>Jiacheng Ye</b>, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.<br />
ICML 2023. [<a href="https://github.com/HKUNLP/icl-ceil">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.02913">OpenICL: An Open-Source Framework for In-context Learning</a><br />
Zhenyu Wu*, YaoXiang Wang*, <b>Jiacheng Ye</b>*, Jiangtao Feng, Jingjing Xu, Yu Qiao, Zhiyong Wu.<br />
ACL 2023, demo. [<a href="https://github.com/Shark-NLP/OpenICL">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2212.10375">Self-adaptive In-context Learning</a><br />
Zhiyong Wu*, Yaoxiang Wang*, <b>Jiacheng Ye</b>*, Lingpeng Kong.<br />
ACL 2023. [<a href="https://github.com/Shark-NLP/self-adaptive-ICL">code</a>]</p>
</li>
</ul>
<p><b>Data Synthesis</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2312.11370">G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</a><br />
Jiahui Gao*, Renjie Pi*, Jipeng Zhang, <b>Jiacheng Ye</b>, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong.<br />
ICLR 2025. [<a href="https://github.com/pipilurj/G-LLaVA">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.13917">Generating Data for Symbolic Language with Large Language Models</a><br />
<b>Jiacheng Ye</b>, Chengzu Li, Lingpeng Kong, Tao Yu.<br />
EMNLP 2023. [<a href="https://github.com/HKUNLP/SymGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=h5OpjGd_lo6">Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning</a><br />
Jiahui Gao*, Renjie Pi*, Yong Lin, Hang Xu, <b>Jiacheng Ye</b>, Zhiyong Wu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong.<br />
ICLR 2023, spotlight. [<a href="https://github.com/HKUNLP/SunGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.12329">ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback</a><br />
<b>Jiacheng Ye</b>, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong.<br />
EMNLP-Findings 2022. [<a href="https://github.com/HKUNLP/ProGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07922">ZeroGen: Efficient Zero-shot Learning via Dataset Generation</a><br />
<b>Jiacheng Ye</b>*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu and Lingpeng Kong.<br />
EMNLP 2022. [<a href="https://github.com/jiacheng-ye/zerogen">code</a>] [<a href="https://www.paperdigest.org/2024/09/most-influential-emnlp-papers-2024-09/">PaperDigest Most Influential Papers</a>]]</p>
</li>
</ul>
<p><b>Understanding LLMs</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.06688">Language Versatilists vs. Specialists: An Empirical
Revisiting on Multilingual Transfer Ability</a><br />
<b>Jiacheng Ye</b>, Xijia Tao, Lingpeng Kong.<br />
Preprint. [<a href="https://github.com/HKUNLP/multilingual-transfer">code</a>]</p>
</li>
</ul>
<p><b>Before LLMs</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2109.04703">Heterogeneous Graph Neural Networks for Keyphrase Generation</a><br />
<b>Jiacheng Ye</b>*, Ruijian Cai*, Tao Gui and Qi Zhang. <br />
EMNLP 2021. [<a href="https://github.com/jiacheng-ye/kg_gater">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2104.08799v2">Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning</a><br />
Yichao Luo*, Yige Xu*, <b>Jiacheng Ye</b>, Xipeng Qiu and Qi Zhang.<br />
EMNLP-Findings 2021. [<a href="https://github.com/xuyige/FGRL4KG">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2103.11441">TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing</a><br />
ACL 2021. [<a href="https://www.textflint.io/textflint">platform</a>] [<a href="https://github.com/textflint/textflint">code</a>] [<a href="https://www.jiqizhixin.com/articles/2021-04-06-6">blog (zh)</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2105.11134">One2Set: Generating Diverse Keyphrases as a Set</a><br />
<b>Jiacheng Ye</b>, Tao Gui, Yichao Luo, Yige Xu, and Qi Zhang. <br />
ACL 2021. [<a href="https://github.com/jiacheng-ye/kg_one2set">code</a>] [<a href="https://mp.weixin.qq.com/s/u6ioHKkju1wXNWwIMFjuaw">blog (zh)</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://www.ijcai.org/proceedings/2020/0550.pdf">Leveraging Document-Level Label Consistency for Named Entity Recognition</a><br />
Tao Gui*, <b>Jiacheng Ye</b>*, Qi Zhang, Yaqian Zhou, Yeyun Gong, Xuanjing Huang. <br />
IJCAI 2020. [<a href="https://github.com/jiacheng-ye/DocL-NER">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://aclanthology.org/2020.emnlp-main.181.pdf">Uncertainty-Aware Label Refinement for Sequence Labeling</a><br />
Tao Gui*, <b>Jiacheng Ye</b>*, Qi Zhang, Zhengyan Li, Zichu Fei, Yeyun Gong and Xuanjing Huang. <br />
EMNLP 2020. [<a href="https://github.com/jiacheng-ye/UANet">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1911.07518">Constructing Multiple Tasks for Augmentation: Improving Neural Image ClassiÔ¨Åcation with K-means Features</a><br />
Tao Gui*, Lizhi Qing*, Qi Zhang, <b>Jiacheng Ye</b>, Hang Yan, Zichu Fei and Xuanjing Huang.<br />
AAAI 2020. [<a href="https://github.com/Howardqlz/Meta-MTL">code</a>] <br /></p>
</li>
</ul>
<h2>Internships</h2>
<ul>
<li><p>Nov. 2021 - Jul. 2023
<br />Research Intern, Shanghai AI Lab.
<br />Mentor: <a href="https://ikekonglp.github.io/">Lingpeng Kong</a>.
<br />Research about Pre-trained Language Model and Text Generation.</p>
</li>
</ul>
<ul>
<li><p>Jun. 2021 - Nov. 2021
<br />Research Intern, Tencent.
<br />Mentor: Zhihui Lao and Lifeng Wang.
<br />Research about a better pre-ranking paradigm for Advertising System and Recommendation System.</p>
</li>
</ul>
<ul>
<li><p>Aug. 2018 - Dec. 2018 
<br />Engineer Intern, Netease.
<br />Worked on data engineering.</p>
</li>
</ul>
<h2>Honors &amp; Awards</h2>
<ul>
<li><p>Outstanding graduate of Shanghai, Shanghai, 2022.</p>
</li>
<li><p>National Scholarship (1%), Ministry of Education of China, 2021.</p>
</li>
<li><p>Glarun Scholarship of CETC-NRIET (5%), Fudan University, 2020.</p>
</li>
<li><p>The second-grade scholarship (10%), Sun Yat-sen University, 2016/2017/2018.</p>
</li>
<li><p>The faculty scholarship (10%), Sun Yat-sen University, 2016/2017.</p>
</li>
</ul>
<h2>Services</h2>
<ul>
<li><p>Conference Reviewer: EMNLP (2022 - 2023), ACL (2022 - 2023), NeurIPS (2023 - 2024), ICLR (2024 - 2025), ICML (2025)</p>
</li>
</ul>
</div>
</body>
</html>
