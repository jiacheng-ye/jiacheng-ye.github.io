<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p><br /></p>
<h2><span style="color:black;font-size:24pt;font-family:Songti SC"><b>Jiacheng Ye</b></span><br /></h2>
<table class="imgtable"><tr><td>
<img src="photos/bio.png" alt="alt text" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p><br />
First-year Ph.D. student<br />
<a href="https://www.cs.hku.hk/">Department of Computer Science</a><br />
<a href="https://www.hku.hk/">The University of Hong Kong</a></p>
<p>[<a href="mailto:carsonye@connect.hku.hk">Email</a>]
[<a href="https://github.com/jiacheng-ye">Github</a>] [<a href="https://scholar.google.com/citations?user=gh0CyxgAAAAJ">Google Scholar</a>] [<a href="https://www.semanticscholar.org/author/Jiacheng-Ye/65846898">Semantic Scholar</a>]</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a first-year Ph.D. student at the Department of Computer Science in the University of Hong Kong (HKU), supervised by <a href="https://ikekonglp.github.io/">Dr. Lingpeng Kong</a> and <a href="https://taoyds.github.io/">Dr. Tao Yu</a> in <a href="https://nlp.cs.hku.hk/">HKUNLP</a>.<br />
I received my Master degree in <a href="https://www.fudan.edu.cn/en/">Fudan University</a>, supervised by  <a href="http://www.qizhang.info/">Prof. Qi Zhang</a> in <a href="https://nlp.fudan.edu.cn/nlpen/main.htm">FudanNLP Group</a>,
and my Bachelor degree in <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>.</p>
<h2>Research</h2>
<p>I am broadly interested in different topics in NLP. Currently, I focus more on interacting and exploiting large language models (LLMs):</p>
<ul>
<li><p>Better interacting with LLMs via In-context Learning (<a href="https://arxiv.org/abs/2212.10375">SAIL</a>, <a href="https://arxiv.org/abs/2302.05698">CEIL</a>, <a href="https://github.com/Shark-NLP/OpenICL">OpenICL</a>)</p>
</li>
<li><p>From general LLMs to task/domain specific efficient models by Data Distillation (<a href="https://arxiv.org/abs/2202.07922">ZeroGen</a>, <a href="https://arxiv.org/abs/2210.12329">ProGen</a>, <a href="https://openreview.net/forum?id=h5OpjGd_lo6">SunGen</a>).</p>
</li>
</ul>
<h2>Publications </h2>
<p>(*: equal contribution)</p>
<p><b>Preprints</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.02913">OpenICL: An Open-Source Framework for In-context Learning</a><br />
Zhenyu Wu*, YaoXiang Wang*, <b>Jiacheng Ye</b>*, Jiangtao Feng, Jingjing Xu, Yu Qiao, Zhiyong Wu.<br />
[<a href="https://arxiv.org/abs/2303.02913">pdf</a>] [<a href="https://github.com/Shark-NLP/OpenICL">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2212.10375">Self-adaptive In-context Learning</a><br />
Zhiyong Wu*, Yaoxiang Wang*, <b>Jiacheng Ye</b>*, Lingpeng Kong.<br />
[<a href="https://arxiv.org/abs/2212.10375">pdf</a>] [<a href="https://github.com/Shark-NLP/self-adaptive-ICL">code</a>]</p>
</li>
</ul>
<p><b>2023</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2302.05698">Compositional Exemplars for In-context Learning</a><br />
<b>Jiacheng Ye</b>, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong.<br />
ICML 2023. [<a href="https://arxiv.org/abs/2302.05698">pdf</a>] [<a href="https://github.com/HKUNLP/icl-ceil">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=h5OpjGd_lo6">Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning</a><br />
Jiahui Gao*, Renjie Pi*, Yong Lin, Hang Xu, <b>Jiacheng Ye</b>, Zhiyong Wu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong.<br />
ICLR 2023, spotlight. [<a href="https://openreview.net/forum?id=h5OpjGd_lo6">pdf</a>] [<a href="https://github.com/HKUNLP/SunGen">code</a>]</p>
</li>
</ul>
<p><b>2022</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2210.12329">ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback</a><br />
<b>Jiacheng Ye</b>, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong.<br />
EMNLP-Findings 2022, long paper. [<a href="https://arxiv.org/abs/2210.12329">pdf</a>] [<a href="https://github.com/HKUNLP/ProGen">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.07922">ZeroGen: Efficient Zero-shot Learning via Dataset Generation</a><br />
<b>Jiacheng Ye</b>*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu and Lingpeng Kong.<br />
EMNLP 2022, long paper. [<a href="https://arxiv.org/abs/2202.07922">pdf</a>]. [<a href="https://github.com/jiacheng-ye/zerogen">code</a>]</p>
</li>
</ul>
<p><b>2021</b></p>
<ul>
<li><p><a href="https://arxiv.org/abs/2109.04703">Heterogeneous Graph Neural Networks for Keyphrase Generation</a><br />
<b>Jiacheng Ye</b>*, Ruijian Cai*, Tao Gui and Qi Zhang. <br />
EMNLP 2021, long paper. [<a href="https://arxiv.org/abs/2109.04703">pdf</a>] [<a href="https://github.com/jiacheng-ye/kg_gater">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2104.08799v2">Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning</a><br />
Yichao Luo*, Yige Xu*, <b>Jiacheng Ye</b>, Xipeng Qiu and Qi Zhang.<br />
EMNLP-Findings 2021, long paper. [<a href="https://arxiv.org/abs/2104.08799v2">pdf</a>] [<a href="https://github.com/xuyige/FGRL4KG">code</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2103.11441">TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing</a><br />
ACL 2021, [<a href="https://arxiv.org/abs/2103.11441">pdf</a>] [<a href="https://www.textflint.io/textflint">platform</a>] [<a href="https://github.com/textflint/textflint">code</a>] [<a href="https://www.jiqizhixin.com/articles/2021-04-06-6">blog (zh)</a>]</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2105.11134">One2Set: Generating Diverse Keyphrases as a Set</a><br />
<b>Jiacheng Ye</b>, Tao Gui, Yichao Luo, Yige Xu, and Qi Zhang. <br />
ACL 2021, long paper. [<a href="https://arxiv.org/abs/2105.11134">pdf</a>] [<a href="https://github.com/jiacheng-ye/kg_one2set">code</a>] [<a href="https://mp.weixin.qq.com/s/u6ioHKkju1wXNWwIMFjuaw">blog (zh)</a>]</p>
</li>
</ul>
<p><b>2020</b></p>
<ul>
<li><p><a href="https://www.ijcai.org/proceedings/2020/0550.pdf">Leveraging Document-Level Label Consistency for Named Entity Recognition</a><br />
Tao Gui*, <b>Jiacheng Ye</b>*, Qi Zhang, Yaqian Zhou, Yeyun Gong, Xuanjing Huang. <br />
IJCAI 2020. [<a href="https://www.ijcai.org/proceedings/2020/0550.pdf">pdf</a>] [<a href="https://github.com/jiacheng-ye/DocL-NER">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://aclanthology.org/2020.emnlp-main.181.pdf">Uncertainty-Aware Label Refinement for Sequence Labeling</a><br />
Tao Gui*, <b>Jiacheng Ye</b>*, Qi Zhang, Zhengyan Li, Zichu Fei, Yeyun Gong and Xuanjing Huang. <br />
EMNLP 2020, long paper. [<a href="https://aclanthology.org/2020.emnlp-main.181.pdf">pdf</a>] [<a href="https://github.com/jiacheng-ye/UANet">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1911.07518">Constructing Multiple Tasks for Augmentation: Improving Neural Image ClassiÔ¨Åcation with K-means Features</a><br />
Tao Gui*, Lizhi Qing*, Qi Zhang, <b>Jiacheng Ye</b>, Hang Yan, Zichu Fei and Xuanjing Huang.<br />
AAAI 2020. [<a href="https://arxiv.org/abs/1911.07518">pdf</a>] [<a href="https://github.com/Howardqlz/Meta-MTL">code</a>] <br /></p>
</li>
</ul>
<h2>Internships</h2>
<ul>
<li><p>Nov. 2021 - Present
<br />Research Intern, Shanghai AI Lab.
<br />Mentor: <a href="https://ikekonglp.github.io/">Lingpeng Kong</a>.
<br />Research about Pre-trained Language Model and Text Generation.</p>
</li>
</ul>
<ul>
<li><p>Jun. 2021 - Nov. 2021
<br />Research Intern, Tencent.
<br />Mentor: Zhihui Lao and Lifeng Wang.
<br />Research about a better pre-ranking paradigm for Advertising System and Recommendation System.</p>
</li>
</ul>
<ul>
<li><p>Aug. 2018 - Dec. 2018 
<br />Engineer Intern, Netease.
<br />Worked on data engineering.</p>
</li>
</ul>
<h2>Honors &amp; Awards</h2>
<ul>
<li><p>Outstanding graduate of Shanghai, Shanghai, 2022.</p>
</li>
<li><p>National Scholarship (1%), Ministry of Education of China, 2021.</p>
</li>
<li><p>Glarun Scholarship of CETC-NRIET (5%), Fudan University, 2020.</p>
</li>
<li><p>The second-grade scholarship (10%), Sun Yat-sen University, 2016/2017/2018.</p>
</li>
<li><p>The faculty scholarship (10%), Sun Yat-sen University, 2016/2017.</p>
</li>
</ul>
</div>
</body>
</html>
